{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 100: Introduction to Data Science\n",
    "\n",
    "## Tutorial 9 - Regression (continued): Class activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "options(repr.matrix.max.rows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the avocado data, which we looked at in week 3, and try to use the small hass volumes of avocados to predict their large hass volumes. To reduce the size of the dataset, let's also narrow our observations to only include avocados from 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this\n",
    "avocado <- read_csv(\"data/avocado_prices.csv\") %>%\n",
    "    filter(yr == 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the quality of our regression model using the RMSPE valueâ€”just like how we used accuracy to evaluate our knn classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the readings, we looked at both RMSE and RMSPE and their differences.<br>\n",
    "* <b>RMSE</b> refers to the root mean squared error, or predicting and evaluating prediction quality on the training data. <br>\n",
    "* <b>RMSPE</b> refers to the root mean squared <b>prediction</b> error, or the error in our predictions made about the actual testing data. We look at this property when we evaluate the quality of our final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at their differences, and at which point in our workflow might we need one over the other. Let's split our data into training and a testing set using a 50-50 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this\n",
    "set.seed(1234)\n",
    "avo_split <- initial_split(avocado, prop = 0.5, strata = large_hass_volume)\n",
    "avo_train <- training(avo_split)\n",
    "avo_test <- testing(avo_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up our recipe, model specification and workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this\n",
    "avo_recipe <- recipe(large_hass_volume ~ small_hass_volume, data = avo_train) %>%\n",
    "                  step_scale(all_predictors()) %>%\n",
    "                  step_center(all_predictors())\n",
    "\n",
    "avo_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "                  set_engine(\"kknn\") %>%\n",
    "                  set_mode(\"regression\")\n",
    "\n",
    "avo_workflow <- workflow() %>%\n",
    "                 add_recipe(avo_recipe) %>%\n",
    "                 add_model(avo_spec)\n",
    "avo_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've provided most of the initial setup: splitting the data into training and testing sets, making the recipe, the model, and adding them to the workflow is done! Now let's perform cross validation with 3 folds and take a look at the RMSPE values. (This might take a bit to run!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "\n",
    "avo_vfold <- vfold_cv(avo_train, v = 3, strata = large_hass_volume)\n",
    "\n",
    "gridvals <- tibble(neighbors = seq(1,200))\n",
    "\n",
    "training_results <- avo_workflow %>%\n",
    "                       tune_grid(resamples = avo_vfold, grid = gridvals) %>%\n",
    "                       collect_metrics() \n",
    "\n",
    "training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look inside the .metric column and you'll find that a given number of neighbors has an observation for each rmse. Now we find the k value that gives the minimum RMSPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avo_min <- training_results %>%\n",
    "               filter(.metric == 'rmse') %>%\n",
    "               filter(mean == min(mean))\n",
    "avo_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimal k value is 18!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using k = 18, fit the model on to our testing set and return the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this\n",
    "avo_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 18) %>%\n",
    "                  set_engine(\"kknn\") %>%\n",
    "                  set_mode(\"regression\")\n",
    "\n",
    "avo_fit <- workflow() %>%\n",
    "           add_recipe(avo_recipe) %>%\n",
    "           add_model(avo_spec) %>%\n",
    "           fit(data = avo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avo_summary <- avo_fit %>% \n",
    "           predict(avo_test) %>%\n",
    "           bind_cols(avo_test) %>%\n",
    "           metrics(truth = large_hass_volume, estimate = .pred) \n",
    "\n",
    "avo_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be fooled here, the number inside the \"rmse\" row now signifies the <b>RMSPE</b>, the error computed when our model is used to actually predict the values of the testing set, i.e. from an <b>out-of-sample</b> prediction. Remember that this value doesn't have a easily-interpretable scale and is measured in reference to the predictor/target variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing with linear regression, find the RMSPE and compare our results to knn regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "lm_spec <- linear_reg() %>%\n",
    "                set_engine(\"lm\") %>%\n",
    "                set_mode(\"regression\")\n",
    "\n",
    "lm_recipe <- recipe(large_hass_volume ~ small_hass_volume, data = avo_train)\n",
    "\n",
    "lm_fit <- workflow() %>%\n",
    "                add_recipe(lm_recipe) %>%\n",
    "                add_model(lm_spec) %>%\n",
    "                fit(data = avo_train)\n",
    "\n",
    "lm_rmse <- lm_fit %>%\n",
    "                predict(avo_train) %>%\n",
    "                bind_cols(avo_train) %>%\n",
    "                metrics(truth = large_hass_volume, estimate = .pred) %>%\n",
    "                filter(.metric == 'rmse') %>%\n",
    "                select(.estimate) %>%\n",
    "                pull()\n",
    "\n",
    "lm_rmse\n",
    "lm_rmspe <- lm_fit %>%\n",
    "                predict(avo_test) %>%\n",
    "                bind_cols(avo_test) %>%\n",
    "                metrics(truth = large_hass_volume, estimate = .pred) %>%\n",
    "                filter(.metric == 'rmse') %>%\n",
    "                select(.estimate) %>%\n",
    "                pull()\n",
    "lm_rmspe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
